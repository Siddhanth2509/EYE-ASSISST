{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f5fef4",
   "metadata": {},
   "source": [
    "# Stage 1 Training Analysis: Binary DR Classification\n",
    "\n",
    "This notebook loads the training history from **Stage 1 (Binary DR Classification)** and visualizes all key metrics including:\n",
    "- Training & Validation Loss curves\n",
    "- Sensitivity (Recall) & Specificity over epochs\n",
    "- Accuracy over epochs\n",
    "- AUC-ROC over epochs\n",
    "- Confusion Matrix from the best checkpoint\n",
    "- Classification Report\n",
    "- Combined metrics dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path (notebooks/phase 3/ -> project root)\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Paths\n",
    "STAGE1_DIR = project_root / 'models' / 'stage1_dr_binary'\n",
    "HISTORY_FILE = STAGE1_DIR / 'training_history.json'\n",
    "BEST_CHECKPOINT = STAGE1_DIR / 'best.pt'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Stage 1 model dir: {STAGE1_DIR}\")\n",
    "print(f\"History file exists: {HISTORY_FILE.exists()}\")\n",
    "print(f\"Best checkpoint exists: {BEST_CHECKPOINT.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f501e7e",
   "metadata": {},
   "source": [
    "## 1. Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "with open(HISTORY_FILE, 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "num_epochs = len(epochs)\n",
    "\n",
    "print(f\"Total epochs trained: {num_epochs}\")\n",
    "print(f\"\\nMetrics tracked:\")\n",
    "for key in history:\n",
    "    print(f\"  - {key}: {len(history[key])} values\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Final Train Loss:     {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss:       {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Best Val Sensitivity: {max(history['val_sensitivity']):.4f} (Epoch {np.argmax(history['val_sensitivity']) + 1})\")\n",
    "print(f\"Best Val Specificity: {max(history['val_specificity']):.4f} (Epoch {np.argmax(history['val_specificity']) + 1})\")\n",
    "print(f\"Best Val Accuracy:    {max(history['val_accuracy']):.4f} (Epoch {np.argmax(history['val_accuracy']) + 1})\")\n",
    "print(f\"Best Val AUC-ROC:     {max(history['val_auc_roc']):.4f} (Epoch {np.argmax(history['val_auc_roc']) + 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157b15c",
   "metadata": {},
   "source": [
    "## 2. Training & Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(epochs, history['train_loss'], 'b-o', linewidth=2, markersize=6, label='Train Loss')\n",
    "ax.plot(epochs, history['val_loss'], 'r-s', linewidth=2, markersize=6, label='Validation Loss')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=13)\n",
    "ax.set_ylabel('Loss (BCE)', fontsize=13)\n",
    "ax.set_title('Stage 1: Training vs Validation Loss', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs)\n",
    "\n",
    "# Annotate min val loss\n",
    "min_val_idx = np.argmin(history['val_loss'])\n",
    "ax.annotate(f\"Min Val Loss: {history['val_loss'][min_val_idx]:.4f}\",\n",
    "            xy=(epochs[min_val_idx], history['val_loss'][min_val_idx]),\n",
    "            xytext=(epochs[min_val_idx]+1, history['val_loss'][min_val_idx]+0.01),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146c982",
   "metadata": {},
   "source": [
    "## 3. Sensitivity & Specificity Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(epochs, history['val_sensitivity'], 'g-o', linewidth=2, markersize=6, label='Sensitivity (Recall / TPR)')\n",
    "ax.plot(epochs, history['val_specificity'], 'm-s', linewidth=2, markersize=6, label='Specificity (TNR)')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=13)\n",
    "ax.set_ylabel('Score', fontsize=13)\n",
    "ax.set_title('Stage 1: Sensitivity vs Specificity', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs)\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "\n",
    "# Highlight best sensitivity\n",
    "best_sens_idx = np.argmax(history['val_sensitivity'])\n",
    "ax.axvline(x=epochs[best_sens_idx], color='green', linestyle='--', alpha=0.5, label=f'Best Sensitivity Epoch ({epochs[best_sens_idx]})')\n",
    "ax.annotate(f\"Best: {history['val_sensitivity'][best_sens_idx]:.4f}\",\n",
    "            xy=(epochs[best_sens_idx], history['val_sensitivity'][best_sens_idx]),\n",
    "            xytext=(epochs[best_sens_idx]-2, history['val_sensitivity'][best_sens_idx]+0.05),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83118d48",
   "metadata": {},
   "source": [
    "## 4. Validation Accuracy Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1bac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(epochs, history['val_accuracy'], 'c-D', linewidth=2, markersize=6, label='Validation Accuracy')\n",
    "ax.fill_between(epochs, history['val_accuracy'], alpha=0.15, color='cyan')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=13)\n",
    "ax.set_ylabel('Accuracy', fontsize=13)\n",
    "ax.set_title('Stage 1: Validation Accuracy', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Annotate best accuracy\n",
    "best_acc_idx = np.argmax(history['val_accuracy'])\n",
    "ax.annotate(f\"Best: {history['val_accuracy'][best_acc_idx]:.4f}\",\n",
    "            xy=(epochs[best_acc_idx], history['val_accuracy'][best_acc_idx]),\n",
    "            xytext=(epochs[best_acc_idx]-2, history['val_accuracy'][best_acc_idx]+0.04),\n",
    "            arrowprops=dict(arrowstyle='->', color='darkcyan'),\n",
    "            fontsize=10, color='darkcyan', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec615e",
   "metadata": {},
   "source": [
    "## 5. AUC-ROC Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd143783",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(epochs, history['val_auc_roc'], 'orange', marker='^', linewidth=2, markersize=7, label='Validation AUC-ROC')\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random Classifier (0.5)')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=13)\n",
    "ax.set_ylabel('AUC-ROC', fontsize=13)\n",
    "ax.set_title('Stage 1: Validation AUC-ROC', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs)\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "\n",
    "# Annotate best AUC\n",
    "best_auc_idx = np.argmax(history['val_auc_roc'])\n",
    "ax.annotate(f\"Best: {history['val_auc_roc'][best_auc_idx]:.4f}\",\n",
    "            xy=(epochs[best_auc_idx], history['val_auc_roc'][best_auc_idx]),\n",
    "            xytext=(epochs[best_auc_idx]-3, history['val_auc_roc'][best_auc_idx]+0.04),\n",
    "            arrowprops=dict(arrowstyle='->', color='darkorange'),\n",
    "            fontsize=10, color='darkorange', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838ceeb",
   "metadata": {},
   "source": [
    "## 6. Combined Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34592f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Stage 1 Training Dashboard: Binary DR Classification', fontsize=16, fontweight='bold', y=1.01)\n",
    "\n",
    "# Plot 1: Loss\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-o', linewidth=2, markersize=5, label='Train')\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-s', linewidth=2, markersize=5, label='Validation')\n",
    "axes[0, 0].set_title('Loss Curves', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('BCE Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(epochs)\n",
    "\n",
    "# Plot 2: Sensitivity & Specificity\n",
    "axes[0, 1].plot(epochs, history['val_sensitivity'], 'g-o', linewidth=2, markersize=5, label='Sensitivity')\n",
    "axes[0, 1].plot(epochs, history['val_specificity'], 'm-s', linewidth=2, markersize=5, label='Specificity')\n",
    "axes[0, 1].set_title('Sensitivity vs Specificity', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(epochs)\n",
    "\n",
    "# Plot 3: Accuracy\n",
    "axes[1, 0].plot(epochs, history['val_accuracy'], 'c-D', linewidth=2, markersize=5, label='Accuracy')\n",
    "axes[1, 0].fill_between(epochs, history['val_accuracy'], alpha=0.15, color='cyan')\n",
    "axes[1, 0].set_title('Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(epochs)\n",
    "\n",
    "# Plot 4: AUC-ROC\n",
    "axes[1, 1].plot(epochs, history['val_auc_roc'], 'orange', marker='^', linewidth=2, markersize=5, label='AUC-ROC')\n",
    "axes[1, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random (0.5)')\n",
    "axes[1, 1].set_title('Validation AUC-ROC', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('AUC-ROC')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(epochs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c5a5b",
   "metadata": {},
   "source": [
    "## 7. Per-Epoch Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build a DataFrame from history\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Train Loss': [f\"{v:.4f}\" for v in history['train_loss']],\n",
    "    'Val Loss': [f\"{v:.4f}\" for v in history['val_loss']],\n",
    "    'Sensitivity': [f\"{v:.4f}\" for v in history['val_sensitivity']],\n",
    "    'Specificity': [f\"{v:.4f}\" for v in history['val_specificity']],\n",
    "    'Accuracy': [f\"{v:.4f}\" for v in history['val_accuracy']],\n",
    "    'AUC-ROC': [f\"{v:.4f}\" for v in history['val_auc_roc']],\n",
    "})\n",
    "df = df.set_index('Epoch')\n",
    "\n",
    "# Highlight the best epoch (by sensitivity)\n",
    "best_epoch = np.argmax(history['val_sensitivity']) + 1\n",
    "print(f\"Best epoch (by sensitivity): Epoch {best_epoch}\")\n",
    "print()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa22d53",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix from Best Checkpoint\n",
    "\n",
    "Load the best checkpoint and extract the confusion matrix metrics (TP, TN, FP, FN) stored in the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d19d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load(BEST_CHECKPOINT, map_location='cpu', weights_only=False)\n",
    "\n",
    "print(\"Checkpoint keys:\", list(checkpoint.keys()))\n",
    "print(f\"\\nBest checkpoint epoch: {checkpoint['epoch']}\")\n",
    "print(f\"Best val sensitivity:  {checkpoint['best_val_sensitivity']:.4f}\")\n",
    "print(f\"\\nMetrics from best checkpoint:\")\n",
    "for k, v in checkpoint['metrics'].items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cea95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix from checkpoint metrics\n",
    "metrics = checkpoint['metrics']\n",
    "tp = metrics.get('true_positives', 0)\n",
    "tn = metrics.get('true_negatives', 0)\n",
    "fp = metrics.get('false_positives', 0)\n",
    "fn = metrics.get('false_negatives', 0)\n",
    "\n",
    "cm = np.array([[tn, fp],\n",
    "               [fn, tp]])\n",
    "\n",
    "class_names = ['NORMAL', 'DR']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "cm_norm = np.nan_to_num(cm_norm)\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Greens', xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Best Model Confusion Matrix (Epoch {checkpoint[\"epoch\"]})', fontsize=15, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Positives  (DR correctly detected):    {tp}\")\n",
    "print(f\"True Negatives  (NORMAL correctly detected):  {tn}\")\n",
    "print(f\"False Positives (NORMAL misclassified as DR): {fp}\")\n",
    "print(f\"False Negatives (DR missed):                  {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77f5d0",
   "metadata": {},
   "source": [
    "## 9. Final Metrics Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of best-epoch metrics\n",
    "metric_names = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'AUC-ROC']\n",
    "metric_values = [\n",
    "    metrics.get('accuracy', 0),\n",
    "    metrics.get('sensitivity', 0),\n",
    "    metrics.get('specificity', 0),\n",
    "    metrics.get('precision', 0),\n",
    "    metrics.get('f1_score', 0),\n",
    "    metrics.get('auc_roc', 0),\n",
    "]\n",
    "\n",
    "colors = ['#2196F3', '#4CAF50', '#9C27B0', '#FF9800', '#F44336', '#FF5722']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(metric_names, metric_values, color=colors, edgecolor='black', linewidth=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, metric_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_ylabel('Score', fontsize=13)\n",
    "ax.set_title(f'Stage 1 Best Model Metrics (Epoch {checkpoint[\"epoch\"]})', fontsize=15, fontweight='bold')\n",
    "ax.axhline(y=0.8, color='gray', linestyle='--', alpha=0.4, label='0.8 threshold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4e5de",
   "metadata": {},
   "source": [
    "## 10. Training Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting gap analysis\n",
    "loss_gap = [v - t for t, v in zip(history['train_loss'], history['val_loss'])]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss gap\n",
    "axes[0].bar(epochs, loss_gap, color=['green' if g > 0 else 'red' for g in loss_gap], alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.8)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Val Loss - Train Loss', fontsize=12)\n",
    "axes[0].set_title('Generalization Gap (Val - Train Loss)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(epochs)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sensitivity-Specificity tradeoff\n",
    "axes[1].scatter(history['val_specificity'], history['val_sensitivity'], \n",
    "                c=epochs, cmap='viridis', s=100, edgecolors='black', zorder=5)\n",
    "for i, ep in enumerate(epochs):\n",
    "    axes[1].annotate(str(ep), (history['val_specificity'][i], history['val_sensitivity'][i]),\n",
    "                     textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "axes[1].set_xlabel('Specificity', fontsize=12)\n",
    "axes[1].set_ylabel('Sensitivity', fontsize=12)\n",
    "axes[1].set_title('Sensitivity-Specificity Tradeoff by Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "cbar.set_label('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"Average generalization gap: {np.mean(loss_gap):.4f}\")\n",
    "print(f\"Final generalization gap:   {loss_gap[-1]:.4f}\")\n",
    "if np.mean(loss_gap) > 0.05:\n",
    "    print(\"Warning: Model may be slightly overfitting (gap > 0.05)\")\n",
    "else:\n",
    "    print(\"Model generalization looks healthy.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
